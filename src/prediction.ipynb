{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from abc import ABC, abstractmethod\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複勝基準モデル作成\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットクラス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, data, is_file=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths (list of str): 学習用データファイルのパスのリスト\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.file = is_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.file:\n",
    "            file_path = self.data[idx]\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            inputs = torch.cat([data[\"race\"], data[\"horse\"], data[\"results\"]])\n",
    "            labels = data[\"label\"]\n",
    "            return inputs, labels\n",
    "        else:\n",
    "            data_set = self.data[idx]\n",
    "            return data_set[\"data\"], data_set[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.fc_in = nn.Linear(input_size, 1024)  # 入力層から隠れ層へ\n",
    "        self.fc1 = nn.Linear(1024, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc_act1 = nn.Mish()\n",
    "        self.fc_act2 = nn.Mish()\n",
    "        self.fc_act3 = nn.Mish()\n",
    "        self.fc_sig = nn.Sigmoid()\n",
    "        self.fc_out = nn.Linear(1024, output_size)  # 隠れ層から出力層へ\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_in(x)\n",
    "        out = self.fc_act1(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc_act2(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc_act3(out)\n",
    "        out = self.fc_out(out)\n",
    "        out = self.fc_sig(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの用意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21765\n",
      "val:4663\n",
      "test:4665\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ファイルで学習させるとき\n",
    "results_path = \"../Processed-Data/Race-Results/\"\n",
    "dir_list_raw = os.listdir(results_path)\n",
    "dir_list = list(map(lambda x: f\"../Processed-Data/Race-Results/{x}\", dir_list_raw))\n",
    "dataset = CustomDataSet(dir_list, is_file=True)\n",
    "\n",
    "\"\"\"\n",
    "results_path = \"../Processed-Data/Race-Results/\"\n",
    "dir_list = os.listdir(results_path)\n",
    "data_set = []\n",
    "for i in tqdm(dir_list):\n",
    "    path = f\"../Processed-Data/Race-Results/{i}\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    data_modify = {\n",
    "        \"data\": torch.cat([data[\"race\"], data[\"horse\"], data[\"results\"]]),\n",
    "        \"label\": data[\"label\"],\n",
    "    }\n",
    "    data_set.append(data_modify)\n",
    "dataset = CustomDataSet(dir_list)\n",
    "\"\"\"\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "# 分割比率を設定 (例: 訓練:検証:テスト = 70%:15%:15%)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "val_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - val_size  # 残りをテストセットとする\n",
    "\n",
    "print(f\"train:{train_size}\")\n",
    "print(f\"val:{val_size}\")\n",
    "print(f\"test:{test_size}\")\n",
    "\n",
    "# データセットをランダムに分割\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "batch = 16\n",
    "# DataLoaderを作成\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各ラベルの分布を調べる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31093 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3256/31093 [00:16<02:24, 192.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m dir_list \u001b[38;5;241m=\u001b[39m get_all_files_in_directory(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Processed-Data/Race-Results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(dir_list):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     20\u001b[0m         data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_all_files_in_directory(path):\n",
    "    \"\"\"\n",
    "    指定したパスの配下にあるすべてのファイル（子孫ファイルも含む）のリストを返します。\n",
    "\n",
    "    :param path: ファイルを検索するディレクトリのパス\n",
    "    :return: ファイルのパスのリスト\n",
    "    \"\"\"\n",
    "    files_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_name in files:\n",
    "            files_list.append(f\"{root}/{file_name}\")\n",
    "    return files_list\n",
    "\n",
    "\n",
    "zero_distribution = torch.zeros(18)\n",
    "one_distribution = torch.zeros(18)\n",
    "dir_list = get_all_files_in_directory(\"../Processed-Data/Race-Results\")\n",
    "for i in tqdm(dir_list):\n",
    "    with open(i, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    labels = data[\"label\"]\n",
    "    # ラベルが1の位置をカウント\n",
    "    one_distribution += labels\n",
    "    # ラベルが0の位置をカウント（全ての要素から1のカウントを引く）\n",
    "    zero_distribution += 1 - labels\n",
    "\n",
    "# 0が1に対してどれくらい多いかの割合を計算\n",
    "zero_to_one_ratio = zero_distribution / one_distribution\n",
    "one_to_zero_ratio = one_distribution / zero_distribution\n",
    "\n",
    "print(\"0が1に対してどれくらい多いかの割合:\\n\", zero_to_one_ratio)\n",
    "print(\"その逆\\n\", one_to_zero_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.0757)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テンソルの平均\n",
    "weight_tensor_one = torch.tensor(\n",
    "    [\n",
    "        4.0467,\n",
    "        3.8973,\n",
    "        4.0508,\n",
    "        3.7718,\n",
    "        4.0142,\n",
    "        3.8689,\n",
    "        4.0264,\n",
    "        4.0231,\n",
    "        4.4075,\n",
    "        4.6175,\n",
    "        5.3236,\n",
    "        5.6824,\n",
    "        7.1395,\n",
    "        8.0703,\n",
    "        9.9560,\n",
    "        12.4485,\n",
    "        85.8520,\n",
    "        96.1656,\n",
    "    ],\n",
    "    dtype=torch.float,\n",
    ")\n",
    "torch.mean(weight_tensor_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15.,\n",
       "        15., 15., 15., 15.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(weight_tensor_one) * 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの用意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.weight_tensor_one = torch.tensor(\n",
    "            [\n",
    "                4.0467,\n",
    "                3.8973,\n",
    "                4.0508,\n",
    "                3.7718,\n",
    "                4.0142,\n",
    "                3.8689,\n",
    "                4.0264,\n",
    "                4.0231,\n",
    "                4.4075,\n",
    "                4.6175,\n",
    "                5.3236,\n",
    "                5.6824,\n",
    "                7.1395,\n",
    "                8.0703,\n",
    "                9.9560,\n",
    "                12.4485,\n",
    "                85.8520,\n",
    "                96.1656,\n",
    "            ],\n",
    "            dtype=torch.float,\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # ラベルが1の要素に対してweight_tensor_oneの重みを適用し、\n",
    "        # ラベルが0の要素に対しては1の重みを適用\n",
    "        weights = torch.where(\n",
    "            targets == 1, self.weight_tensor_one, torch.ones_like(targets)\n",
    "        )\n",
    "        weights = weights.to(targets.device)  # weightsをtargetsと同じデバイスに移動\n",
    "        # 重み付きバイナリクロスエントロピー損失の計算\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets, weight=weights)\n",
    "        return loss\n",
    "\n",
    "\n",
    "model = NNClassifier(1534, 18).to(device)\n",
    "criterion = WeightedBCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500]  Loss1.3499157428741455\n",
      "Accuracy for label 1: 27.46%\n",
      "Accuracy for label 0: 85.24%\n",
      "Epoch [20/500]  Loss1.0067671537399292\n",
      "Accuracy for label 1: 31.61%\n",
      "Accuracy for label 0: 85.97%\n",
      "Epoch [30/500]  Loss1.066394567489624\n",
      "Accuracy for label 1: 35.12%\n",
      "Accuracy for label 0: 86.59%\n",
      "Epoch [40/500]  Loss0.9392207860946655\n",
      "Accuracy for label 1: 36.92%\n",
      "Accuracy for label 0: 86.91%\n",
      "Epoch [50/500]  Loss0.9865322709083557\n",
      "Accuracy for label 1: 36.89%\n",
      "Accuracy for label 0: 86.91%\n",
      "Epoch [60/500]  Loss0.9213132262229919\n",
      "Accuracy for label 1: 36.92%\n",
      "Accuracy for label 0: 86.91%\n",
      "Epoch [70/500]  Loss1.3121676445007324\n",
      "Accuracy for label 1: 36.57%\n",
      "Accuracy for label 0: 86.85%\n",
      "Epoch [80/500]  Loss0.9469384551048279\n",
      "Accuracy for label 1: 36.15%\n",
      "Accuracy for label 0: 86.77%\n",
      "Epoch [90/500]  Loss0.9151275157928467\n",
      "Accuracy for label 1: 36.19%\n",
      "Accuracy for label 0: 86.78%\n",
      "Epoch [100/500]  Loss0.9598709344863892\n",
      "Accuracy for label 1: 36.04%\n",
      "Accuracy for label 0: 86.76%\n",
      "Epoch [110/500]  Loss0.8541965484619141\n",
      "Accuracy for label 1: 36.07%\n",
      "Accuracy for label 0: 86.76%\n",
      "Epoch [120/500]  Loss0.9362922310829163\n",
      "Accuracy for label 1: 35.90%\n",
      "Accuracy for label 0: 86.73%\n",
      "Epoch [130/500]  Loss1.0049461126327515\n",
      "Accuracy for label 1: 35.84%\n",
      "Accuracy for label 0: 86.72%\n",
      "Epoch [140/500]  Loss0.8772907853126526\n",
      "Accuracy for label 1: 35.55%\n",
      "Accuracy for label 0: 86.67%\n",
      "Epoch [150/500]  Loss1.2819257974624634\n",
      "Accuracy for label 1: 35.98%\n",
      "Accuracy for label 0: 86.74%\n",
      "Epoch [160/500]  Loss0.9020541906356812\n",
      "Accuracy for label 1: 35.99%\n",
      "Accuracy for label 0: 86.75%\n",
      "Epoch [170/500]  Loss0.8234484791755676\n",
      "Accuracy for label 1: 36.00%\n",
      "Accuracy for label 0: 86.75%\n",
      "Epoch [180/500]  Loss1.154419183731079\n",
      "Accuracy for label 1: 36.06%\n",
      "Accuracy for label 0: 86.76%\n",
      "Epoch [190/500]  Loss0.8681351542472839\n",
      "Accuracy for label 1: 35.78%\n",
      "Accuracy for label 0: 86.71%\n",
      "Epoch [200/500]  Loss0.9079630970954895\n",
      "Accuracy for label 1: 35.69%\n",
      "Accuracy for label 0: 86.69%\n",
      "Epoch [210/500]  Loss0.8625885844230652\n",
      "Accuracy for label 1: 35.88%\n",
      "Accuracy for label 0: 86.73%\n",
      "Epoch [220/500]  Loss0.8172643184661865\n",
      "Accuracy for label 1: 35.96%\n",
      "Accuracy for label 0: 86.74%\n",
      "Epoch [230/500]  Loss0.8900254964828491\n",
      "Accuracy for label 1: 35.87%\n",
      "Accuracy for label 0: 86.72%\n",
      "Epoch [240/500]  Loss0.9177362322807312\n",
      "Accuracy for label 1: 35.79%\n",
      "Accuracy for label 0: 86.71%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m, in \u001b[0;36mCustomDataSet.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile:\n\u001b[0;32m     16\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m---> 17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorse\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def check_accuracy_topk(loader, model, k=3):\n",
    "    num_correct_1 = 0\n",
    "    num_samples_1 = 0\n",
    "    num_correct_0 = 0\n",
    "    num_samples_0 = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            scores = model(x)\n",
    "            # 上位k個の予測を1に、それ以外を0にする\n",
    "            topk_predictions = torch.zeros_like(scores, device=device)\n",
    "            topk_vals, topk_indices = scores.topk(k, dim=1)\n",
    "            # 上位k個の位置に1を設定\n",
    "            topk_predictions.scatter_(1, topk_indices, 1)\n",
    "            # 1の場合\n",
    "            correct_predictions_1 = topk_predictions.bool() & y.bool()\n",
    "            num_correct_1 += correct_predictions_1.type(torch.float).sum().item()\n",
    "            num_samples_1 += y.sum().item()\n",
    "            # 0の場合\n",
    "            correct_predictions_0 = (~topk_predictions.bool()) & (~y.bool())\n",
    "            num_correct_0 += correct_predictions_0.type(torch.float).sum().item()\n",
    "            num_samples_0 += (1 - y).sum().item()\n",
    "        # 正解率の計算\n",
    "        accuracy_1 = (num_correct_1 / num_samples_1 * 100) if num_samples_1 > 0 else 0\n",
    "        accuracy_0 = (num_correct_0 / num_samples_0 * 100) if num_samples_0 > 0 else 0\n",
    "    model.train()\n",
    "    return f\"Accuracy for label 1: {accuracy_1:.2f}%\\nAccuracy for label 0: {accuracy_0:.2f}%\"\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        x = data.to(device)\n",
    "        y = targets.to(device)\n",
    "        scores = model(x)\n",
    "        loss = criterion(scores, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}]  Loss{loss}\\n{check_accuracy_topk(val_loader, model)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy for label 1: 35.36%\\nAccuracy for label 0: 86.65%'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_accuracy_topk(loader, model, k=3):\n",
    "    num_correct_1 = 0\n",
    "    num_samples_1 = 0\n",
    "    num_correct_0 = 0\n",
    "    num_samples_0 = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            scores = model(x)\n",
    "            # 上位k個の予測を1に、それ以外を0にする\n",
    "            topk_predictions = torch.zeros_like(scores, device=device)\n",
    "            topk_vals, topk_indices = scores.topk(k, dim=1)\n",
    "            # 上位k個の位置に1を設定\n",
    "            topk_predictions.scatter_(1, topk_indices, 1)\n",
    "            # 1の場合\n",
    "            correct_predictions_1 = topk_predictions.bool() & y.bool()\n",
    "            num_correct_1 += correct_predictions_1.type(torch.float).sum().item()\n",
    "            num_samples_1 += y.sum().item()\n",
    "            # 0の場合\n",
    "            correct_predictions_0 = (~topk_predictions.bool()) & (~y.bool())\n",
    "            num_correct_0 += correct_predictions_0.type(torch.float).sum().item()\n",
    "            num_samples_0 += (1 - y).sum().item()\n",
    "        # 正解率の計算\n",
    "        accuracy_1 = (num_correct_1 / num_samples_1 * 100) if num_samples_1 > 0 else 0\n",
    "        accuracy_0 = (num_correct_0 / num_samples_0 * 100) if num_samples_0 > 0 else 0\n",
    "    model.train()\n",
    "    return f\"Accuracy for label 1: {accuracy_1:.2f}%\\nAccuracy for label 0: {accuracy_0:.2f}%\"\n",
    "\n",
    "\n",
    "\n",
    "print(check_accuracy_topk(test_loader, model, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル保存(状態のみ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/nn_classifier_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アンサンブル NN モデル\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットクラス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, data, is_file=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths (list of str): 学習用データファイルのパスのリスト\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.file = is_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.file:\n",
    "            file_path = self.data[idx]\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            inputs = torch.cat([data[\"race\"], data[\"horse\"], data[\"results\"]])\n",
    "            labels = data[\"label\"]\n",
    "            return inputs, labels\n",
    "        else:\n",
    "            data_set = self.data[idx]\n",
    "            return data_set[\"data\"], data_set[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.rand_num = random.randint(8, 64)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc_in = nn.Linear(input_size, self.rand_num)  # 入力層から隠れ層へ\n",
    "        self.fc1 = nn.Linear(self.rand_num, self.rand_num)\n",
    "        self.fc2 = nn.Linear(self.rand_num, self.rand_num)\n",
    "        self.fc_act1 = nn.Mish()\n",
    "        self.fc_act2 = nn.Mish()\n",
    "        self.fc_act3 = nn.Mish()\n",
    "        self.fc_sig = nn.Sigmoid()\n",
    "        self.fc_out = nn.Linear(self.rand_num, output_size)  # 隠れ層から出力層へ\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_in(x)\n",
    "        out = self.fc_act1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc_act2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc_act3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc_out(out)\n",
    "        out = self.fc_sig(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの用意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21765\n",
      "val:4663\n",
      "test:4665\n",
      "Sub train loader 0: 4353 samples\n",
      "Sub train loader 1: 4353 samples\n",
      "Sub train loader 2: 4353 samples\n",
      "Sub train loader 3: 4353 samples\n",
      "Sub train loader 4: 4353 samples\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ファイルで学習させるとき\n",
    "results_path = \"../Processed-Data/Race-Results/\"\n",
    "dir_list_raw = os.listdir(results_path)\n",
    "dir_list = list(map(lambda x: f\"../Processed-Data/Race-Results/{x}\", dir_list_raw))\n",
    "dataset_top = CustomDataSet(dir_list, is_file=True)\n",
    "\n",
    "dataset_size = len(dataset_top)\n",
    "# 分割比率を設定 (例: 訓練:検証:テスト = 70%:15%:15%)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "val_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - val_size  # 残りをテストセットとする\n",
    "\n",
    "print(f\"train:{train_size}\")\n",
    "print(f\"val:{val_size}\")\n",
    "print(f\"test:{test_size}\")\n",
    "\n",
    "# データセットをランダムに分割\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset_top, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# 訓練データセットを5つのサブセットに分割\n",
    "sub_train_sizes = [int(train_size / 5) for _ in range(4)]  # 最初の4つのサイズ\n",
    "sub_train_sizes.append(train_size - sum(sub_train_sizes))  # 最後のサブセットのサイズ\n",
    "\n",
    "batch = 16\n",
    "\n",
    "# 分割された訓練データセットのインデックスを生成\n",
    "indices = torch.randperm(train_size).tolist()\n",
    "sub_train_datasets = [\n",
    "    Subset(\n",
    "        train_dataset, indices[sum(sub_train_sizes[:i]) : sum(sub_train_sizes[: i + 1])]\n",
    "    )\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "# 各サブセットに対応するDataLoaderを作成\n",
    "sub_train_loaders = [\n",
    "    DataLoader(dataset, batch_size=batch, shuffle=True, pin_memory=True)\n",
    "    for dataset in sub_train_datasets\n",
    "]\n",
    "\n",
    "# 各サブセットのDataLoaderを確認\n",
    "for i, loader in enumerate(sub_train_loaders):\n",
    "    print(f\"Sub train loader {i}: {len(loader.dataset)} samples\")\n",
    "\n",
    "# 検証、確認用のDataLoaderを作成\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独自の誤差関数とモデルの用意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.weight_tensor_one = torch.tensor(\n",
    "            [\n",
    "                4.0467,\n",
    "                3.8973,\n",
    "                4.0508,\n",
    "                3.7718,\n",
    "                4.0142,\n",
    "                3.8689,\n",
    "                4.0264,\n",
    "                4.0231,\n",
    "                4.4075,\n",
    "                4.6175,\n",
    "                5.3236,\n",
    "                5.6824,\n",
    "                7.1395,\n",
    "                8.0703,\n",
    "                9.9560,\n",
    "                12.4485,\n",
    "                85.8520,\n",
    "                96.1656,\n",
    "            ],\n",
    "            dtype=torch.float,\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # ラベルが1の要素に対してweight_tensor_oneの重みを適用し、\n",
    "        # ラベルが0の要素に対しては1の重みを適用\n",
    "        weights = torch.where(\n",
    "            targets == 1, self.weight_tensor_one, torch.ones_like(targets)\n",
    "        )\n",
    "        weights = weights.to(targets.device)  # weightsをtargetsと同じデバイスに移動\n",
    "        # 重み付きバイナリクロスエントロピー損失の計算\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets, weight=weights)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# アンサンブル用にモデルを５個作る\n",
    "models = [NNClassifier(1534, 18).to(device) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アンサンブル学習させる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, Epoch 1 finished\n",
      "Model 0, Epoch 2 finished\n",
      "Model 0, Epoch 3 finished\n",
      "Model 0, Epoch 4 finished\n",
      "Model 0, Epoch 5 finished\n",
      "Model 1, Epoch 1 finished\n",
      "Model 1, Epoch 2 finished\n",
      "Model 1, Epoch 3 finished\n",
      "Model 1, Epoch 4 finished\n",
      "Model 1, Epoch 5 finished\n",
      "Model 2, Epoch 1 finished\n",
      "Model 2, Epoch 2 finished\n",
      "Model 2, Epoch 3 finished\n",
      "Model 2, Epoch 4 finished\n",
      "Model 2, Epoch 5 finished\n",
      "Model 3, Epoch 1 finished\n",
      "Model 3, Epoch 2 finished\n",
      "Model 3, Epoch 3 finished\n",
      "Model 3, Epoch 4 finished\n",
      "Model 3, Epoch 5 finished\n",
      "Model 4, Epoch 1 finished\n",
      "Model 4, Epoch 2 finished\n",
      "Model 4, Epoch 3 finished\n",
      "Model 4, Epoch 4 finished\n",
      "Model 4, Epoch 5 finished\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for count, model in enumerate(models):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    criterion = WeightedBCELoss().to(device)  # 損失関数も適切なデバイスに移動\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            x = data.to(device)\n",
    "            y = targets.to(device)\n",
    "            scores = model(x)\n",
    "            loss = criterion(scores, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(f\"Model {count}, Epoch {epoch + 1} finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for label 1: 22.78%\n",
      "Accuracy for label 0: 84.41%\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy_topk_ensemble(loaders, models, k=3):\n",
    "    num_correct_1 = 0\n",
    "    num_samples_1 = 0\n",
    "    num_correct_0 = 0\n",
    "    num_samples_0 = 0\n",
    "\n",
    "    # モデルを評価モードに設定\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loaders:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # アンサンブルの予測を格納するリスト\n",
    "            predictions = []\n",
    "            for model in models:\n",
    "                scores = model(x)\n",
    "                predictions.append(scores)\n",
    "            # 予測の平均を計算\n",
    "            avg_predictions = torch.mean(torch.stack(predictions), dim=0)\n",
    "\n",
    "            # 上位k個の予測を1に、それ以外を0にする\n",
    "            topk_predictions = torch.zeros_like(avg_predictions, device=device)\n",
    "            topk_vals, topk_indices = avg_predictions.topk(k, dim=1)\n",
    "            topk_predictions.scatter_(1, topk_indices, 1)\n",
    "\n",
    "            # 1の場合\n",
    "            correct_predictions_1 = topk_predictions.bool() & y.bool()\n",
    "            num_correct_1 += correct_predictions_1.type(torch.float).sum().item()\n",
    "            num_samples_1 += y.sum().item()\n",
    "            # 0の場合\n",
    "            correct_predictions_0 = (~topk_predictions.bool()) & (~y.bool())\n",
    "            num_correct_0 += correct_predictions_0.type(torch.float).sum().item()\n",
    "            num_samples_0 += (1 - y).sum().item()\n",
    "\n",
    "    # 正解率の計算\n",
    "    accuracy_1 = (num_correct_1 / num_samples_1 * 100) if num_samples_1 > 0 else 0\n",
    "    accuracy_0 = (num_correct_0 / num_samples_0 * 100) if num_samples_0 > 0 else 0\n",
    "\n",
    "    # モデルを訓練モードに戻す\n",
    "    for model in models:\n",
    "        model.train()\n",
    "\n",
    "    return f\"Accuracy for label 1: {accuracy_1:.2f}%\\nAccuracy for label 0: {accuracy_0:.2f}%\"\n",
    "\n",
    "\n",
    "# 使用例\n",
    "print(check_accuracy_topk_ensemble(test_loader, models, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, model in enumerate(models):\n",
    "    # モデルの状態と乱数の値を保存\n",
    "    model_info = {\"state_dict\": model.state_dict(), \"rand_num\": model.rand_num}\n",
    "    torch.save(model_info, f\"../models/nn_ensemble_{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    # モデル情報のロード\n",
    "    model_info = torch.load(f\"../models/nn_ensemble_{i}\")\n",
    "    rand_num = model_info[\"rand_num\"]\n",
    "\n",
    "    # モデルのインスタンスを作成（適切なinput_sizeとoutput_sizeを指定）\n",
    "    model = NNClassifier(1534, 18)\n",
    "    model.rand_num = rand_num  # 乱数の値を設定\n",
    "\n",
    "    # モデルの状態をロード\n",
    "    model.load_state_dict(model_info[\"state_dict\"])\n",
    "    models.append(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
