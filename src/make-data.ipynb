{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レース結果を整形するクラス(仮)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceResultsProcessor_:\n",
    "    \"\"\"レース結果をデータを整形する\"\"\"\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        self.results_raw = pd.read_pickle(path)\n",
    "        self.results_processed = pd.read_pickle(path)\n",
    "        # データの0埋めを行う\n",
    "        self.results_processed.fillna(0, inplace=True)\n",
    "        # 馬体重のカラムについては「0(0)」で埋める\n",
    "        self.results_processed[\"馬体重\"].replace(0, \"0(0)\", inplace=True)\n",
    "\n",
    "    def drop_columns(self, columns: [str]) -> None:\n",
    "        \"\"\"不要なカラムを削除\"\"\"\n",
    "        self.results_processed = self.results_processed.drop(columns=columns)\n",
    "\n",
    "    def divide_weight_gender(self):\n",
    "        \"\"\"馬の性齢と馬体重を分割する\"\"\"\n",
    "        self.results_processed[\"性別\"] = self.results_processed[\"性齢\"].str[0]\n",
    "        self.results_processed[\"年齢\"] = self.results_processed[\"性齢\"].str[1:]\n",
    "        self.results_processed[\"体重\"] = self.results_processed[\"馬体重\"].replace(\n",
    "            to_replace=r\"(\\d+).*\", value=r\"\\1\", regex=True\n",
    "        )\n",
    "        self.results_processed[\"増減\"] = self.results_processed[\"馬体重\"].replace(\n",
    "            to_replace=r\"\\d+\\(\\+{0,1}([-]{0,1}\\d+)\\)\", value=r\"\\1\", regex=True\n",
    "        )\n",
    "\n",
    "    def transform_rank(self):\n",
    "        self.results_processed[\"3着以内\"] = self.results_processed[\"着順\"].apply(\n",
    "            lambda x: 1 if isinstance(x, int) and x <= 3 else 0\n",
    "        )\n",
    "\n",
    "    def transform_date(self, date: str):\n",
    "        \"\"\"日付を変換して、その年の1月1日からの日数を計算する\"\"\"\n",
    "        # 日付の形式を変換\n",
    "        date_converted = datetime.datetime.strptime(date, \"%Y年%m月%d日\")\n",
    "        # その年の1月1日を計算\n",
    "        base_date = datetime.datetime(date_converted.year, 1, 1)\n",
    "        # 日数の差を計算\n",
    "        return (date_converted - base_date).days\n",
    "\n",
    "    def __extraction_drop_columns(\n",
    "        self, df: pd.DataFrame, columns: [str]\n",
    "    ) -> (pd.DataFrame, pd.DataFrame):\n",
    "        df_extraction = df.loc[:, columns]\n",
    "        df_dropped = df.drop(columns=columns)\n",
    "        return df_extraction, df_dropped\n",
    "\n",
    "    def make_race_infos(self):\n",
    "        \"\"\"データをレースの情報、出走馬の情報、過去成績の3つに分ける\"\"\"\n",
    "        drop_columns = [\n",
    "            \"馬名\",\n",
    "            \"性齢\",\n",
    "            \"騎手\",\n",
    "            \"タイム\",\n",
    "            \"着差\",\n",
    "            \"人気\",\n",
    "            \"調教師\",\n",
    "            \"単勝\",\n",
    "            \"jockey_id\",\n",
    "            \"馬体重\",\n",
    "        ]\n",
    "        race_info_columns = [\n",
    "            \"date\",\n",
    "            \"round\",\n",
    "            \"course_length\",\n",
    "            \"course_type\",\n",
    "            \"course_way\",\n",
    "            \"weather\",\n",
    "            \"state_grass\",\n",
    "            \"state_dirt\",\n",
    "            \"place\",\n",
    "            \"class\",\n",
    "        ]\n",
    "        self.divide_weight_gender()\n",
    "        self.transform_rank()\n",
    "        self.drop_columns(drop_columns)\n",
    "        self.race_info, self.horse_info = self.__extraction_drop_columns(\n",
    "            self.results_processed, race_info_columns\n",
    "        )\n",
    "        self.horse_results, self.horse_info = self.__extraction_drop_columns(\n",
    "            self.horse_info, [\"horse_id\"]\n",
    "        )\n",
    "\n",
    "        self.race_info = self.race_info.loc[[0], :]\n",
    "        self.race_info[\"date\"] = self.transform_date(self.race_info.loc[0, \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>round</th>\n",
       "      <th>course_length</th>\n",
       "      <th>course_type</th>\n",
       "      <th>course_way</th>\n",
       "      <th>weather</th>\n",
       "      <th>state_grass</th>\n",
       "      <th>state_dirt</th>\n",
       "      <th>place</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td>ダ</td>\n",
       "      <td>右</td>\n",
       "      <td>晴</td>\n",
       "      <td>無</td>\n",
       "      <td>良</td>\n",
       "      <td>札幌</td>\n",
       "      <td>1勝</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date round course_length course_type course_way weather state_grass  \\\n",
       "0   239     7          1000           ダ          右       晴           無   \n",
       "\n",
       "  state_dirt place class  \n",
       "0          良    札幌    1勝  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = RaceResultsProcessor_(\"../Raw-Data/Race-Results/2022/01020607.pkl\")\n",
    "test.make_race_infos()\n",
    "\n",
    "test.race_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3456/3456 [00:29<00:00, 115.51it/s]\n"
     ]
    }
   ],
   "source": [
    "results_path = \"../Raw-Data/Race-Results/2022/\"\n",
    "dir_list = os.listdir(results_path)\n",
    "df_list = []\n",
    "for i in tqdm(dir_list):\n",
    "    result = RaceResultsProcessor_(f\"{results_path}{i}\")\n",
    "    result.make_race_infos()\n",
    "    df_list.append(result.race_info)\n",
    "\n",
    "df_integration = pd.concat(df_list)\n",
    "df_integration.to_pickle(\"../tmp/race-infos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'round', 'course_length', 'course_type', 'course_way',\n",
      "       'weather', 'state_grass', 'state_dirt', 'place', 'class'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>round</th>\n",
       "      <th>course_length</th>\n",
       "      <th>course_type</th>\n",
       "      <th>course_way</th>\n",
       "      <th>weather</th>\n",
       "      <th>state_grass</th>\n",
       "      <th>state_dirt</th>\n",
       "      <th>place</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>1800</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>曇</td>\n",
       "      <td>重</td>\n",
       "      <td>良</td>\n",
       "      <td>札幌</td>\n",
       "      <td>未勝利</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>ダ</td>\n",
       "      <td>右</td>\n",
       "      <td>曇</td>\n",
       "      <td>良</td>\n",
       "      <td>重</td>\n",
       "      <td>札幌</td>\n",
       "      <td>未勝利</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>曇</td>\n",
       "      <td>重</td>\n",
       "      <td>良</td>\n",
       "      <td>札幌</td>\n",
       "      <td>未勝利</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>4</td>\n",
       "      <td>1200</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>曇</td>\n",
       "      <td>重</td>\n",
       "      <td>良</td>\n",
       "      <td>札幌</td>\n",
       "      <td>未勝利</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>5</td>\n",
       "      <td>1700</td>\n",
       "      <td>ダ</td>\n",
       "      <td>右</td>\n",
       "      <td>曇</td>\n",
       "      <td>良</td>\n",
       "      <td>重</td>\n",
       "      <td>札幌</td>\n",
       "      <td>新馬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246</td>\n",
       "      <td>12</td>\n",
       "      <td>2600</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>晴</td>\n",
       "      <td>良</td>\n",
       "      <td>良</td>\n",
       "      <td>小倉</td>\n",
       "      <td>1勝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246</td>\n",
       "      <td>12</td>\n",
       "      <td>2600</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>晴</td>\n",
       "      <td>良</td>\n",
       "      <td>良</td>\n",
       "      <td>小倉</td>\n",
       "      <td>1勝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246</td>\n",
       "      <td>12</td>\n",
       "      <td>2600</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>晴</td>\n",
       "      <td>良</td>\n",
       "      <td>重</td>\n",
       "      <td>小倉</td>\n",
       "      <td>1勝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246</td>\n",
       "      <td>12</td>\n",
       "      <td>2600</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>晴</td>\n",
       "      <td>良</td>\n",
       "      <td>稍</td>\n",
       "      <td>小倉</td>\n",
       "      <td>1勝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246</td>\n",
       "      <td>12</td>\n",
       "      <td>2600</td>\n",
       "      <td>芝</td>\n",
       "      <td>右</td>\n",
       "      <td>晴</td>\n",
       "      <td>良</td>\n",
       "      <td>不</td>\n",
       "      <td>小倉</td>\n",
       "      <td>1勝</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3461 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    date round course_length course_type course_way weather state_grass  \\\n",
       "0    203     1          1800           芝          右       曇           重   \n",
       "0    203     2          1700           ダ          右       曇           良   \n",
       "0    203     3          1500           芝          右       曇           重   \n",
       "0    203     4          1200           芝          右       曇           重   \n",
       "0    203     5          1700           ダ          右       曇           良   \n",
       "..   ...   ...           ...         ...        ...     ...         ...   \n",
       "0    246    12          2600           芝          右       晴           良   \n",
       "0    246    12          2600           芝          右       晴           良   \n",
       "0    246    12          2600           芝          右       晴           良   \n",
       "0    246    12          2600           芝          右       晴           良   \n",
       "0    246    12          2600           芝          右       晴           良   \n",
       "\n",
       "   state_dirt place class  \n",
       "0           良    札幌   未勝利  \n",
       "0           重    札幌   未勝利  \n",
       "0           良    札幌   未勝利  \n",
       "0           良    札幌   未勝利  \n",
       "0           重    札幌    新馬  \n",
       "..        ...   ...   ...  \n",
       "0           良    小倉    1勝  \n",
       "0           良    小倉    1勝  \n",
       "0           重    小倉    1勝  \n",
       "0           稍    小倉    1勝  \n",
       "0           不    小倉    1勝  \n",
       "\n",
       "[3461 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_integration.columns)\n",
    "df_integration[df_integration[\"course_type\"] == \"障\"]\n",
    "\"\"\"\n",
    "無              1693\n",
    "良              1385\n",
    "重               359\n",
    "稍                18\n",
    "不                 1\n",
    "\"\"\"\n",
    "# 足りないデータを追加\n",
    "tmps = []\n",
    "tmps.append(df_integration)\n",
    "for i in [\"無\", \"良\", \"重\", \"稍\", \"不\"]:\n",
    "    tmp = df_integration[-1:].copy()\n",
    "    tmp[\"state_dirt\"] = i\n",
    "    tmps.append(tmp)\n",
    "\n",
    "df_integration_new = pd.concat(tmps)\n",
    "# いらないデータを置換\n",
    "df_integration_new[\"course_way\"].replace(\"無\", \"右\", inplace=True)\n",
    "df_integration_new[\"state_grass\"].replace(\"無\", \"良\", inplace=True)\n",
    "df_integration_new[\"state_dirt\"].replace(\"無\", \"良\", inplace=True)\n",
    "df_integration_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'round', 'course_length', 'course_type_芝', 'course_type_ダ',\n",
       "       'course_type_障', 'course_way_右', 'course_way_左', 'course_way_直',\n",
       "       'weather_曇', 'weather_晴', 'weather_雨', 'weather_雪', 'state_grass_重',\n",
       "       'state_grass_良', 'state_grass_稍', 'state_grass_不', 'state_dirt_良',\n",
       "       'state_dirt_重', 'state_dirt_稍', 'state_dirt_不', 'place_札幌', 'place_函館',\n",
       "       'place_福島', 'place_新潟', 'place_東京', 'place_中山', 'place_中京', 'place_阪神',\n",
       "       'place_小倉', 'class_未勝利', 'class_新馬', 'class_1勝', 'class_2勝', 'class_3勝',\n",
       "       'class_G3', 'class_L', 'class_オープン', 'class_G2', 'class_G1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ワンホットエンコーディングを行うカラムを指定\n",
    "columns_to_encode = [\n",
    "    \"course_type\",\n",
    "    \"course_way\",\n",
    "    \"weather\",\n",
    "    \"state_grass\",\n",
    "    \"state_dirt\",\n",
    "    \"place\",\n",
    "    \"class\",\n",
    "]\n",
    "\n",
    "# エンコーダーのインスタンスを作成\n",
    "encoder = ce.OneHotEncoder(\n",
    "    cols=columns_to_encode, use_cat_names=True, handle_unknown=\"value\"\n",
    ")\n",
    "\n",
    "# ワンホットエンコーディングを実行\n",
    "df_integration_encoded = encoder.fit_transform(df_integration_new)\n",
    "with open(\"../models/race_info_encoder.pickle\", \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "df_integration_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 出走馬情報の標準化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceResults:\n",
    "    \"\"\"レース結果をデータを整形する\"\"\"\n",
    "\n",
    "    with open(\"../models/race_info_encoder.pickle\", \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        self.results_raw = pd.read_pickle(path)\n",
    "        self.results_processed = pd.read_pickle(path)\n",
    "        # データの0埋めを行う\n",
    "        self.results_processed.fillna(0, inplace=True)\n",
    "        # 馬体重のカラムについては「0(0)」で埋める\n",
    "        self.results_processed[\"馬体重\"].replace(0, \"0(0)\", inplace=True)\n",
    "        with open(\"../models/race_info_encoder.pickle\", \"rb\") as f:\n",
    "            self.encoder = pickle.load(f)\n",
    "\n",
    "    def read_df(path: str) -> pd.DataFrame:\n",
    "        if not isinstance(path, str):\n",
    "            raise TypeError(\n",
    "                f'\"path\" argument is expected to be of type str, got {type(path).__name__} instead'\n",
    "            )\n",
    "        results_processed = pd.read_pickle(path)\n",
    "        return results_processed\n",
    "\n",
    "    def divide_weight_gender(df_raw: pd.DataFrame):\n",
    "        \"\"\"馬の性齢と馬体重を分割する\"\"\"\n",
    "        df = df_raw.copy()\n",
    "        gender = df[\"性齢\"].str[0]\n",
    "        df[\"牡\"] = gender.map(lambda x: 1 if x == \"牡\" else 0)\n",
    "        df[\"牝\"] = gender.map(lambda x: 1 if x == \"牝\" else 0)\n",
    "        df[\"セ\"] = gender.map(lambda x: 1 if x == \"セ\" else 0)\n",
    "        df[\"年齢\"] = df[\"性齢\"].str[1:]\n",
    "        df[\"体重\"] = df[\"馬体重\"].replace(\n",
    "            to_replace=r\"(\\d+).*\", value=r\"\\1\", regex=True\n",
    "        )\n",
    "        df[\"増減\"] = df[\"馬体重\"].replace(\n",
    "            to_replace=r\"\\d+\\(\\+{0,1}([-]{0,1}\\d+)\\)\", value=r\"\\1\", regex=True\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def transform_rank(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df_raw.copy()\n",
    "        df[\"3着以内\"] = df[\"着順\"].apply(\n",
    "            lambda x: 1 if isinstance(x, int) and x <= 3 else 0\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def drop_columns(df_raw: pd.DataFrame, columns: [str]) -> pd.DataFrame:\n",
    "        \"\"\"不要なカラムを削除\"\"\"\n",
    "        df = df_raw.drop(columns=columns)\n",
    "        return df\n",
    "\n",
    "    def transform_date(date: str):\n",
    "        \"\"\"日付を変換して、その年の1月1日からの週数を計算する\"\"\"\n",
    "        # 日付の形式を変換\n",
    "        date_converted = datetime.datetime.strptime(date, \"%Y年%m月%d日\")\n",
    "        # その年の1月1日を計算\n",
    "        base_date = datetime.datetime(date_converted.year, 1, 1)\n",
    "        # 週数の差を計算\n",
    "        return (date_converted - base_date).days // 7\n",
    "\n",
    "    def extraction_drop_columns(\n",
    "        df: pd.DataFrame, columns: [str]\n",
    "    ) -> (pd.DataFrame, pd.DataFrame):\n",
    "        df_extraction = df.loc[:, columns]\n",
    "        df_dropped = df.drop(columns=columns)\n",
    "        return df_extraction, df_dropped\n",
    "\n",
    "    @classmethod\n",
    "    def adapt_race_info(cls, df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df_raw.loc[[0], :]\n",
    "        df[\"date\"] = cls.transform_date(df.loc[0, \"date\"])\n",
    "        df[\"course_length\"] = float(df.loc[0, \"course_length\"]) / 100\n",
    "\n",
    "        df = cls.encoder.transform(df)\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def make_infos(cls, path: str) -> {}:\n",
    "        drop_columns = [\n",
    "            \"馬名\",\n",
    "            \"性齢\",\n",
    "            \"騎手\",\n",
    "            \"タイム\",\n",
    "            \"着差\",\n",
    "            \"人気\",\n",
    "            \"調教師\",\n",
    "            \"単勝\",\n",
    "            \"jockey_id\",\n",
    "            \"馬体重\",\n",
    "            \"着順\",\n",
    "        ]\n",
    "        race_info_columns = [\n",
    "            \"date\",\n",
    "            \"round\",\n",
    "            \"course_length\",\n",
    "            \"course_type\",\n",
    "            \"course_way\",\n",
    "            \"weather\",\n",
    "            \"state_grass\",\n",
    "            \"state_dirt\",\n",
    "            \"place\",\n",
    "            \"class\",\n",
    "        ]\n",
    "        df = cls.read_df(path)\n",
    "        # データの0埋めを行う\n",
    "        df = df.fillna(0)\n",
    "        # 馬体重のカラムについては「0(0)」で埋める\n",
    "        df[\"馬体重\"].replace(0, \"0(0)\", inplace=True)\n",
    "        df[\"馬体重\"].replace(\"計不\", \"0(0)\", inplace=True)\n",
    "        df = cls.divide_weight_gender(df)\n",
    "        df = cls.transform_rank(df)\n",
    "        df = cls.drop_columns(df, drop_columns)\n",
    "        race_info, horse_info = cls.extraction_drop_columns(df, race_info_columns)\n",
    "        horse_id, horse_info = cls.extraction_drop_columns(horse_info, [\"horse_id\"])\n",
    "\n",
    "        # 標準化等の変換\n",
    "        race_info = cls.adapt_race_info(race_info)\n",
    "        return {\n",
    "            \"race\": race_info,\n",
    "            \"horse\": horse_info,\n",
    "            \"ids\": list(horse_id.iloc[:, 0].values),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>着順</th>\n",
       "      <th>枠番</th>\n",
       "      <th>馬番</th>\n",
       "      <th>斤量</th>\n",
       "      <th>性別</th>\n",
       "      <th>年齢</th>\n",
       "      <th>体重</th>\n",
       "      <th>増減</th>\n",
       "      <th>3着以内</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>牡</td>\n",
       "      <td>3</td>\n",
       "      <td>486</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>牝</td>\n",
       "      <td>3</td>\n",
       "      <td>476</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>セ</td>\n",
       "      <td>4</td>\n",
       "      <td>478</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>牝</td>\n",
       "      <td>5</td>\n",
       "      <td>470</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>牡</td>\n",
       "      <td>4</td>\n",
       "      <td>538</td>\n",
       "      <td>-18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>牡</td>\n",
       "      <td>5</td>\n",
       "      <td>498</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>牝</td>\n",
       "      <td>3</td>\n",
       "      <td>442</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>牝</td>\n",
       "      <td>3</td>\n",
       "      <td>442</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>牡</td>\n",
       "      <td>3</td>\n",
       "      <td>510</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>52</td>\n",
       "      <td>牝</td>\n",
       "      <td>3</td>\n",
       "      <td>514</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>55</td>\n",
       "      <td>セ</td>\n",
       "      <td>5</td>\n",
       "      <td>490</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>牡</td>\n",
       "      <td>4</td>\n",
       "      <td>486</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    着順  枠番  馬番  斤量 性別 年齢   体重   増減  3着以内\n",
       "0    6   1   1  51  牡  3  486    4     0\n",
       "1    9   2   2  52  牝  3  476    6     0\n",
       "2    7   3   3  54  セ  4  478    8     0\n",
       "3   11   4   4  55  牝  5  470    6     0\n",
       "4    3   5   5  55  牡  4  538  -18     1\n",
       "5    8   5   6  57  牡  5  498    2     0\n",
       "6    4   6   7  51  牝  3  442    0     0\n",
       "7    1   6   8  52  牝  3  442   10     1\n",
       "8    5   7   9  53  牡  3  510    4     0\n",
       "9   12   7  10  52  牝  3  514    2     0\n",
       "10   2   8  11  55  セ  5  490    0     1\n",
       "11  10   8  12  57  牡  4  486    4     0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = RaceResults.make_infos(\"../Raw-Data/Race-Results/2022/01020607.pkl\")\n",
    "test[\"horse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../Raw-Data/Race-Results/2022/\"\n",
    "dir_list = os.listdir(results_path)\n",
    "df_list = []\n",
    "for i in tqdm(dir_list):\n",
    "    result = RaceResults.make_infos(f\"{results_path}{i}\")\n",
    "    df_list.append(result[\"horse\"])\n",
    "\n",
    "df_integration = pd.concat(df_list)\n",
    "df_integration.to_pickle(\"../tmp/horse-info.pkl\")\n",
    "df_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../tmp/horse-info.pkl\")\n",
    "scaler = StandardScaler()\n",
    "# 標準化したいカラムを指定\n",
    "columns_to_scale = [\"体重\", \"増減\"]\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/horse_info_scaler.pickle\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットクラス\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # div_termの計算\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim)\n",
    "        )\n",
    "\n",
    "        # position * div_term のサイズ調整\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, nheads, nlayers, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            hidden_dim, nheads, hidden_dim, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_linear(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_dim, nheads, nlayers, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, dropout)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(\n",
    "            hidden_dim, nheads, hidden_dim, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, nlayers)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_decoder(src, src)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, nheads, nlayers, dropout=0.1):\n",
    "        super(TransformerVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, nheads, nlayers, dropout)\n",
    "        self.decoder = Decoder(hidden_dim, input_dim, nheads, nlayers, dropout)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_out = nn.Linear(latent_dim, hidden_dim)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, src):\n",
    "        encoded = self.encoder(src)\n",
    "        mu = self.fc_mu(encoded)\n",
    "        log_var = self.fc_log_var(encoded)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        z = self.fc_out(z)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, log_var\n",
    "\n",
    "    def get_latent_val(self, src):\n",
    "        encoded = self.encoder(src)\n",
    "        val = self.fc_mu(encoded)\n",
    "        return val\n",
    "\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, path: str, device=None) -> None:\n",
    "        \"\"\"VAEモデルを推論モードで立ち上げる\n",
    "\n",
    "        Args:\n",
    "            str: 保存したモデルのパス\n",
    "            device (_type_, optional): cpuでモデルを使う場合は\"cpu\"を入れる. デフォルトはNone.\n",
    "        \"\"\"\n",
    "        self.model = TransformerVAE(\n",
    "            input_dim=67,\n",
    "            hidden_dim=64,\n",
    "            latent_dim=4,\n",
    "            nheads=8,\n",
    "            nlayers=8,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        if device == \"cpu\":\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            )\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def transform(self, df_raw: pd.DataFrame) -> torch.Tensor:\n",
    "        \"\"\"VAEによる変換をする\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象のデータフレーム\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 変換し１次元にしたデータ\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        df = df.iloc[:].astype(\"float32\")\n",
    "        # データフレームをテンソルに変換\n",
    "        data = torch.tensor(df.values, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded = self.model.get_latent_val(data)\n",
    "        return torch.flatten(encoded)\n",
    "\n",
    "    def process(self, dfs: list[pd.DataFrame]):\n",
    "        processed_data = []\n",
    "        for i in dfs:\n",
    "            data_transform = self.transform(i)\n",
    "            processed_data.append(data_transform)\n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 成績の処理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スーパークラス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultProcessor(ABC):\n",
    "    @abstractmethod\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    def read_results(path: str) -> pd.DataFrame:\n",
    "        if isinstance(path, str):\n",
    "            return pd.read_pickle(path)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f'\"path\" argument is expected to be of type str, got {type(path).__name__} instead'\n",
    "            )\n",
    "\n",
    "    def arrange_result(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"データフレームの欠損値の0埋めとカラム名の空白を消す\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 処理後のデータフレーム\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        # 欠損値を0埋め\n",
    "        df = df.fillna(0)\n",
    "        # カラム名の空白を削除\n",
    "        df.columns = df.columns.str.replace(\" \", \"\")\n",
    "        df[\"日付\"] = pd.to_datetime(df[\"日付\"], format=\"%Y/%m/%d\")\n",
    "        return df\n",
    "\n",
    "    def remove_str(x: any) -> str:\n",
    "        \"\"\"文字列から数値だけを取り出す\n",
    "\n",
    "        Args:\n",
    "            x (any): 処理する文字列\n",
    "\n",
    "        Returns:\n",
    "            str: 処理後の文字列（数字がなかった場合は\"0\"を返す）\n",
    "        \"\"\"\n",
    "        x_str = str(x)\n",
    "        is_contain_num = re.search(r\"\\d+\", x_str)\n",
    "        if is_contain_num:\n",
    "            return is_contain_num.group()\n",
    "        else:\n",
    "            return \"0\"\n",
    "\n",
    "    def convert_date(x: str) -> int:\n",
    "        \"\"\"日付をその年の1/1から数えた週数に変換する\n",
    "\n",
    "        Args:\n",
    "            x (str): 日付\n",
    "\n",
    "        Returns:\n",
    "            int: 週数\n",
    "        \"\"\"\n",
    "\n",
    "        # その年の1月1日を計算\n",
    "        base_date = datetime.datetime(x.year, 1, 1)\n",
    "        # 週数の差を計算\n",
    "        return (x - base_date).days // 7\n",
    "\n",
    "    def transform_held(held: str) -> str:\n",
    "        \"\"\"開催場所の文字列から不要な文字を取り除く。中央以外は\"その他\"にする\n",
    "\n",
    "        Args:\n",
    "            held (str): 処理する文字列\n",
    "\n",
    "        Returns:\n",
    "            str: 処理後の文字列\n",
    "        \"\"\"\n",
    "        trim_held = re.sub(r\"\\d*\", \"\", held)\n",
    "        if not trim_held in [\n",
    "            \"東京\",\n",
    "            \"中山\",\n",
    "            \"中京\",\n",
    "            \"阪神\",\n",
    "            \"札幌\",\n",
    "            \"函館\",\n",
    "            \"福島\",\n",
    "            \"新潟\",\n",
    "            \"京都\",\n",
    "            \"小倉\",\n",
    "        ]:\n",
    "            return \"その他\"\n",
    "        return trim_held\n",
    "\n",
    "    def transform_race_name(race: str) -> str:\n",
    "        \"\"\"レースのクラス分けをする。当てはまらないものは\"その他\"にする\n",
    "\n",
    "        Args:\n",
    "            race (str): レースクラスの文字列\n",
    "\n",
    "        Returns:\n",
    "            str: 処理後文字列\n",
    "        \"\"\"\n",
    "\n",
    "        if re.search(r\".*(新馬|未勝利|1勝|2勝|3勝|OP|G1|G2|G3|L).*\", race):\n",
    "            transform_name = re.sub(\n",
    "                r\".*(新馬|未勝利|1勝|2勝|3勝|OP|G1|G2|G3|L).*\", r\"\\1\", race\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            transform_name = \"その他\"\n",
    "        return transform_name\n",
    "\n",
    "    def extract_addition(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        weight = df[\"馬体重\"]\n",
    "\n",
    "        addition = weight.map(lambda x: re.sub(r\".*\\(([+-]\\d{1,3}|0)\\).*\", r\"\\1\", x))\n",
    "        addition = addition.map(lambda x: re.sub(r\"\\+\", \"\", x))\n",
    "        return addition\n",
    "\n",
    "    def drop_columns(df: pd.DataFrame, columns: [str]) -> pd.DataFrame:\n",
    "        df_processed = df.drop(\n",
    "            columns,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return df_processed\n",
    "\n",
    "    def divide_corse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_divided = df\n",
    "\n",
    "        df_divided[\"コース\"] = df_divided[\"距離\"].map(lambda x: x[0])\n",
    "        df_divided[\"距離\"] = df_divided[\"距離\"].map(lambda x: int(x[1:]) / 100)\n",
    "        return df_divided\n",
    "\n",
    "    def divide_horse_weight(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"馬体重と増減を分ける\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 対象のデータフレーム\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 処理後のデータフレーム\n",
    "        \"\"\"\n",
    "        df_divided = df\n",
    "\n",
    "        df_divided[\"馬体重\"] = df_divided[\"馬体重\"].map(\n",
    "            lambda x: x.replace(\"計不\", \"0(0)\")\n",
    "        )\n",
    "        weight = df_divided[\"馬体重\"]\n",
    "        weight_addition = weight.map(\n",
    "            lambda x: re.sub(r\".*\\(([+-]\\d{1,3}|0)\\).*\", r\"\\1\", x)\n",
    "        )\n",
    "        weight_addition = weight_addition.map(lambda x: re.sub(r\"\\+\", \"\", x))\n",
    "        df_divided[\"増減\"] = weight_addition\n",
    "        df_divided[\"馬体重\"] = df_divided[\"馬体重\"].map(\n",
    "            lambda x: re.sub(r\"\\([+-]*\\d+\\)\", \"\", x)\n",
    "        )\n",
    "\n",
    "        return df_divided\n",
    "\n",
    "    def add_rows(df_raw: pd.DataFrame, rows: int) -> pd.DataFrame:\n",
    "        \"\"\"行の補填をする\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "            rows (int): 補填する行数\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 処理後のデータフレーム\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame(np.zeros((rows, len(df.columns))), columns=df.columns),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 親成績の処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedigreeResults(ResultProcessor):\n",
    "    with open(\"../models/pedigree_pca.pickle\", \"rb\") as f:\n",
    "        pca = pickle.load(f)\n",
    "\n",
    "    def arrange_result(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"データフレームの欠損値の0埋めとカラム名の空白を消す\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 処理後のデータフレーム\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        # 欠損値を0埋め\n",
    "        df = df.fillna(0)\n",
    "        # カラム名の空白を削除\n",
    "        df.columns = df.columns.str.replace(\" \", \"\")\n",
    "        # df[\"日付\"] = pd.to_datetime(df[\"日付\"], format=\"%Y/%m/%d\")\n",
    "        return df\n",
    "\n",
    "    def read_results(path: str) -> [pd.DataFrame]:\n",
    "        if isinstance(path, str):\n",
    "            ped_results = []\n",
    "            with open(path, \"rb\") as f:\n",
    "                peds = pickle.load(f)\n",
    "            for i in peds:\n",
    "                df = pd.read_pickle(f\"../Raw-Data/Pedigree-Results/{i}.pkl\")\n",
    "                ped_results.append(df)\n",
    "            return ped_results\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f'\"path\" argument is expected to be of type str, got {type(path).__name__} instead'\n",
    "            )\n",
    "\n",
    "    def transform_race_length(length: str | int | float) -> str:\n",
    "        if isinstance(length, str):\n",
    "            # 距離の記載がない場合に対応\n",
    "            if re.match(r\"\\d+\", length):\n",
    "                length: int = int(length)\n",
    "            else:\n",
    "                length: int = 0\n",
    "        elif math.isnan(length):\n",
    "            length: int = 0\n",
    "        elif not isinstance(length, int):\n",
    "            raise TypeError(\n",
    "                f'\"length\" argument is expected to be of type int or str, got {type(length).__name__} instead. The value is {length}'\n",
    "            )\n",
    "        match length:\n",
    "            case length if length < 1000:\n",
    "                return \"不明\"\n",
    "            case length if length <= 1300:\n",
    "                return \"S\"\n",
    "            case length if length <= 1899:\n",
    "                return \"M\"\n",
    "            case length if length <= 2100:\n",
    "                return \"I\"\n",
    "            case length if length <= 2700:\n",
    "                return \"L\"\n",
    "            case length if length > 2700:\n",
    "                return \"E\"\n",
    "\n",
    "    def delete_invalid_race(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.drop(index=df[df[\"着順\"] == 0].index)\n",
    "        df = df.drop(index=df[df[\"着順\"] == \"0\"].index)\n",
    "        return df\n",
    "\n",
    "    def divide_corse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_divided = df\n",
    "        match_str = r\"[芝ダ障]{0,1}\\d{1,4}\"\n",
    "        if len(df_divided) <= 1:\n",
    "            df_divided[\"コース\"] = 0\n",
    "            df_divided[\"距離\"] = 0\n",
    "            return df_divided\n",
    "        df_divided[\"コース\"] = df_divided[\"距離\"].map(\n",
    "            lambda x: x[0] if not isinstance(x, int) else 0\n",
    "        )\n",
    "        df_divided[\"距離\"] = df_divided[\"距離\"].map(\n",
    "            lambda x: x[1:] if not isinstance(x, int) else 0\n",
    "        )\n",
    "        return df_divided\n",
    "\n",
    "    def totalling_result(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"成績データを競馬場や馬場、着順等で分けて集計する\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 成績データ（時系列順）\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 集計した成績データ\n",
    "        \"\"\"\n",
    "        df_tmp: pd.DataFrame = pd.read_pickle(\n",
    "            \"../template/pedigree_results_template.pkl\"\n",
    "        )\n",
    "        for _, row in df.iterrows():\n",
    "            col: list[str] = [row[\"距離\"], row[\"コース\"]]\n",
    "            if \"不明\" in col:\n",
    "                continue\n",
    "            rank: str = row[\"着順\"] if int(row[\"着順\"]) <= 3 else \"3<\"\n",
    "            state: str = row[\"馬場\"] if row[\"馬場\"] != \"不明\" else \"良\"\n",
    "            race_type: str = (\n",
    "                \"重賞\" if row[\"レース名\"] in [\"G3\", \"G1\", \"G2\"] else \"非重賞\"\n",
    "            )\n",
    "            col = f'{row[\"開催\"]}_{race_type}_{row[\"距離\"]}_{row[\"コース\"]}_{state}_{rank}'\n",
    "            df_tmp[col] += 1\n",
    "        return df_tmp\n",
    "\n",
    "    @classmethod\n",
    "    def modify(cls, df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df_raw.copy()\n",
    "        df = cls.arrange_result(df)\n",
    "        # 加工\n",
    "        df = df[[\"開催\", \"天気\", \"レース名\", \"着順\", \"距離\", \"馬場\"]]\n",
    "        df = cls.divide_corse(df)\n",
    "        if len(df) > 1:\n",
    "            df[\"開催\"] = df[\"開催\"].map(cls.transform_held)\n",
    "            df[\"レース名\"] = df[\"レース名\"].map(cls.transform_race_name)\n",
    "            df[\"距離\"] = df[\"距離\"].map(cls.transform_race_length)\n",
    "            df[\"着順\"] = df[\"着順\"].map(cls.remove_str)\n",
    "        df = cls.delete_invalid_race(df)\n",
    "        df = df.replace(0, \"不明\")\n",
    "        return df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    @classmethod\n",
    "    def pca_transform(cls, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_transform = cls.pca.transform(df)\n",
    "        return pd.DataFrame(df_transform)\n",
    "\n",
    "    @classmethod\n",
    "    def transform(cls, path: str) -> pd.DataFrame:\n",
    "        results_raws = cls.read_results(path)\n",
    "        results = []\n",
    "        for i in results_raws:\n",
    "            results_df = cls.modify(i)\n",
    "            results.append(cls.totalling_result(results_df))\n",
    "        result = pd.concat(results, axis=1)\n",
    "        return cls.pca_transform(result)\n",
    "\n",
    "    @classmethod\n",
    "    def process(cls, path: [str] or str) -> pd.DataFrame:\n",
    "        if isinstance(path, list):\n",
    "            results = []\n",
    "            for i in path:\n",
    "                results.append(cls.transform(i))\n",
    "            result = pd.concat(results)\n",
    "            rows = 18 - len(result)\n",
    "            if rows > 0:\n",
    "                result = cls.add_rows(result, rows)\n",
    "            return result\n",
    "        elif isinstance(path, str):\n",
    "            return cls.transform(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 過去成績の処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResult(ResultProcessor):\n",
    "    max_column = 10\n",
    "    columns_to_scale = [\"馬体重\", \"増減\", \"斤量\"]\n",
    "    with open(\"../models/horse_result_encoder.pickle\", \"rb\") as f:\n",
    "        encoder: ce.OneHotEncoder = pickle.load(f)\n",
    "    with open(\"../models/horse_results_scaler.pickle\", \"rb\") as f:\n",
    "        scaler: StandardScaler = pickle.load(f)\n",
    "\n",
    "    def select_newer_race(df_raw: pd.DataFrame, date: str) -> pd.DataFrame:\n",
    "        \"\"\"レース日より前の日付の成績を抽出する\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "            date (str): 基準にする日付\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 抽出したデータ\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        df = df[df[\"日付\"] < datetime.datetime.strptime(date, \"%Y年%m月%d日\")]\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def mapping_data(cls, df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"データフレームの各カラムの値に対する一括処理をまとめた関数\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 処理後のデータフレーム\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        df[\"日付\"] = df[\"日付\"].map(cls.convert_date)\n",
    "        df[\"開催\"] = df[\"開催\"].map(cls.transform_held)\n",
    "        df[\"レース名\"] = df[\"レース名\"].map(cls.transform_race_name)\n",
    "        df[\"馬番\"] = df[\"馬番\"].map(lambda x: 0 if x > 18 else x)\n",
    "        df[\"着順\"] = df[\"着順\"].map(cls.remove_str)\n",
    "        df[\"馬場\"] = df[\"馬場\"].replace(0, \"不明\")\n",
    "        df[\"天気\"] = df[\"天気\"].replace(0, \"不明\")\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def transform(cls, df_raw: pd.DataFrame, date: str) -> pd.DataFrame:\n",
    "        \"\"\"成績データを変換する\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 成績データ\n",
    "            date (str): 基準の日付。これより前のレース成績だけを扱う\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 変換後データ\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        columns = [\n",
    "            \"賞金\",\n",
    "            \"厩舎ｺﾒﾝﾄ\",\n",
    "            \"備考\",\n",
    "            \"勝ち馬(2着馬)\",\n",
    "            \"着差\",\n",
    "            \"ﾀｲﾑ指数\",\n",
    "            \"通過\",\n",
    "            \"ペース\",\n",
    "            \"上り\",\n",
    "            \"馬場指数\",\n",
    "            \"タイム\",\n",
    "            \"映像\",\n",
    "            \"騎手\",\n",
    "            \"オッズ\",\n",
    "            \"人気\",\n",
    "        ]\n",
    "        df = cls.arrange_result(df)\n",
    "        if date:\n",
    "            df = cls.select_newer_race(df, date)\n",
    "        df = cls.drop_columns(df, columns)\n",
    "        df = cls.divide_horse_weight(df)\n",
    "        df = cls.divide_corse(df)\n",
    "        df = cls.mapping_data(df)\n",
    "        # 型をintにする\n",
    "        df = df.astype({\"R\": int, \"枠番\": int})\n",
    "        # 標準化\n",
    "        df[cls.columns_to_scale] = cls.scaler.transform(df[cls.columns_to_scale])\n",
    "        # ダミー変数化\n",
    "        df = cls.encoder.transform(df)\n",
    "        # 行数の調整\n",
    "        shortage_rows = cls.max_column - len(df)\n",
    "        if shortage_rows > 0:\n",
    "            df = cls.add_rows(df, shortage_rows)\n",
    "        df = df.head(cls.max_column)\n",
    "        df = df.iloc[::-1].reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def process(\n",
    "        cls, path: list[str] | str, date=None\n",
    "    ) -> list[pd.DataFrame] | pd.DataFrame:\n",
    "        \"\"\"成績データが複数かどうかで処理を分けるための関数\n",
    "\n",
    "        Args:\n",
    "            path (list[str]or str): 成績データのファイルパス\n",
    "            date (_type_, optional): 日付。デフォルトはNone.\n",
    "\n",
    "        Returns:\n",
    "            list[pd.DataFrame] or pd.DataFrame: 変換したデータはデータフレーム単一か、リストに入れて返す\n",
    "        \"\"\"\n",
    "        if isinstance(path, list):\n",
    "            dfs = []\n",
    "            for i in path:\n",
    "                df_raw = cls.read_results(i)\n",
    "                df = df_raw.copy()\n",
    "                df = cls.transform(df, date)\n",
    "                dfs.append(df)\n",
    "            return dfs\n",
    "        else:\n",
    "            df_raw = cls.read_results(path)\n",
    "            df = df_raw.copy()\n",
    "            df = cls.transform(df, date)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レース結果の処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceResults:\n",
    "    \"\"\"レース結果をデータを整形する\"\"\"\n",
    "\n",
    "    with open(\"../models/race_info_encoder.pickle\", \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    with open(\"../models/horse_info_scaler.pickle\", \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    def read_df(path: str) -> pd.DataFrame:\n",
    "        \"\"\"データフレームの読み込み\n",
    "\n",
    "        Args:\n",
    "            path (str): pickleのパス\n",
    "\n",
    "        Raises:\n",
    "            TypeError: 引数が文字列でなければエラーを出す\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 読み込んだデータフレーム\n",
    "        \"\"\"\n",
    "        if not isinstance(path, str):\n",
    "            raise TypeError(\n",
    "                f'\"path\" argument is expected to be of type str, got {type(path).__name__} instead'\n",
    "            )\n",
    "        results_processed = pd.read_pickle(path)\n",
    "        return results_processed\n",
    "\n",
    "    def divide_weight_gender(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"性齢の値を性別と年齢に分け、馬体重も体重と増減に分ける。性別はダミー変数化する\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 変換後データフレーム\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        gender = df[\"性齢\"].str[0]\n",
    "        df[\"牡\"] = gender.map(lambda x: 1 if x == \"牡\" else 0)\n",
    "        df[\"牝\"] = gender.map(lambda x: 1 if x == \"牝\" else 0)\n",
    "        df[\"セ\"] = gender.map(lambda x: 1 if x == \"セ\" else 0)\n",
    "        df[\"年齢\"] = df[\"性齢\"].str[1:]\n",
    "        df[\"体重\"] = df[\"馬体重\"].replace(\n",
    "            to_replace=r\"(\\d+).*\", value=r\"\\1\", regex=True\n",
    "        )\n",
    "        df[\"増減\"] = df[\"馬体重\"].replace(\n",
    "            to_replace=r\"\\d+\\(\\+{0,1}([-]{0,1}\\d+)\\)\", value=r\"\\1\", regex=True\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def transform_rank(df_raw: pd.DataFrame, multi=True) -> pd.DataFrame:\n",
    "        \"\"\"着順のデータを３着以内かどうかの値にする。(3着以内であれば1、そうでなければ0)\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 変換後データ\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        threshold = 3**multi\n",
    "        df[\"3着以内\"] = df[\"着順\"].apply(\n",
    "            lambda x: 1 if isinstance(x, int) and x <= threshold else 0\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def drop_columns(df_raw: pd.DataFrame, columns: [str]) -> pd.DataFrame:\n",
    "        \"\"\"不要なカラムを削除する\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 対象データフレーム\n",
    "            columns (str]): 削除するカラム名\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 削除後データフレーム\n",
    "        \"\"\"\n",
    "        df = df_raw.drop(columns=columns)\n",
    "        return df\n",
    "\n",
    "    def transform_date(date: str) -> str:\n",
    "        \"\"\"日付を変換して、その年の1月1日からの週数を計算する\n",
    "\n",
    "        Args:\n",
    "            date (str): 日付の文字列（%Y年%m月%d日）\n",
    "\n",
    "        Returns:\n",
    "            str: 変換後の日付文字列\n",
    "        \"\"\"\n",
    "        # 日付の形式を変換\n",
    "        date_converted = datetime.datetime.strptime(date, \"%Y年%m月%d日\")\n",
    "        # その年の1月1日を計算\n",
    "        base_date = datetime.datetime(date_converted.year, 1, 1)\n",
    "        # 週数の差を計算\n",
    "        return (date_converted - base_date).days // 7\n",
    "\n",
    "    def extraction_drop_columns(\n",
    "        df: pd.DataFrame, columns: [str]\n",
    "    ) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"データフレームをカラム指定で分割する\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 対象のデータフレーム\n",
    "            pd ([str]): 分割するカラム名\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame, pd.DataFrame): 指定したカラムを抽出したデータフレームと、それを取り除いたデータフレーム\n",
    "        \"\"\"\n",
    "        df_extraction = df.loc[:, columns]\n",
    "        df_dropped = df.drop(columns=columns)\n",
    "        return df_extraction, df_dropped\n",
    "\n",
    "    def add_rows(df_raw: pd.DataFrame, rows: int) -> pd.DataFrame:\n",
    "        df = df_raw.copy()\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame(np.zeros((rows, len(df.columns))), columns=df.columns),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def adapt_race_info(cls, df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"レース情報の日付をその年の週数に、コースの長さのスケールを1/100にする。データ型も変更する\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): レース情報\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 変換後のデータ\n",
    "        \"\"\"\n",
    "        df = df_raw.loc[[0], :]\n",
    "        df[\"date\"] = cls.transform_date(df.loc[0, \"date\"])\n",
    "        df[\"course_length\"] = float(df.loc[0, \"course_length\"]) / 100\n",
    "        df[\"round\"] = df[\"round\"].astype(float)\n",
    "\n",
    "        df = cls.encoder.transform(df)\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def horse_info_transform(cls, df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"出走馬情報の標準化と足りない行の補填、型変換をする\n",
    "\n",
    "        Args:\n",
    "            df_raw (pd.DataFrame): 出走馬情報\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 変換後のデータ\n",
    "        \"\"\"\n",
    "        df = df_raw.copy()\n",
    "        columns_to_scale = [\"体重\", \"増減\"]\n",
    "        df[columns_to_scale] = cls.scaler.transform(df[columns_to_scale])\n",
    "        shortage_rows = 18 - len(df)\n",
    "        df = cls.add_rows(df, shortage_rows)\n",
    "        df[\"年齢\"] = df[\"年齢\"].astype(float)\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def make_infos(cls, path: str, multi=True) -> {pd.DataFrame or str}:\n",
    "        \"\"\"レース結果をレース情報、出走馬情報、出走馬ID、レース日付、正解ラベルの5個に分ける\n",
    "\n",
    "        Args:\n",
    "            path (str): レース結果ファイルのパス\n",
    "\n",
    "        Returns:\n",
    "            {pd.DataFrame or str}: dictで保存。キーはそれぞれrace,horse,ids,date,label。date以外はデータフレーム\n",
    "        \"\"\"\n",
    "        drop_columns = [\n",
    "            \"馬名\",\n",
    "            \"性齢\",\n",
    "            \"騎手\",\n",
    "            \"タイム\",\n",
    "            \"着差\",\n",
    "            \"人気\",\n",
    "            \"調教師\",\n",
    "            \"単勝\",\n",
    "            \"jockey_id\",\n",
    "            \"馬体重\",\n",
    "            \"着順\",\n",
    "        ]\n",
    "        race_info_columns = [\n",
    "            \"date\",\n",
    "            \"round\",\n",
    "            \"course_length\",\n",
    "            \"course_type\",\n",
    "            \"course_way\",\n",
    "            \"weather\",\n",
    "            \"state_grass\",\n",
    "            \"state_dirt\",\n",
    "            \"place\",\n",
    "            \"class\",\n",
    "        ]\n",
    "        df_raw = cls.read_df(path)\n",
    "        df = df_raw.copy()\n",
    "        # データの0埋めを行う\n",
    "        df = df.fillna(0)\n",
    "        # 馬体重のカラムについては「0(0)」で埋める\n",
    "        df[\"馬体重\"].replace(0, \"0(0)\", inplace=True)\n",
    "        df[\"馬体重\"].replace(\"計不\", \"0(0)\", inplace=True)\n",
    "        df = cls.divide_weight_gender(df)\n",
    "        df = cls.transform_rank(df, multi=multi)\n",
    "        df = cls.drop_columns(df, drop_columns)\n",
    "        race_info, horse_info = cls.extraction_drop_columns(df, race_info_columns)\n",
    "        horse_id, horse_info = cls.extraction_drop_columns(horse_info, [\"horse_id\"])\n",
    "\n",
    "        # 標準化等の変換\n",
    "        race_info = cls.adapt_race_info(race_info)\n",
    "        horse_info = cls.horse_info_transform(horse_info)\n",
    "        return {\n",
    "            \"race\": race_info,\n",
    "            \"horse\": horse_info.drop([\"3着以内\"], axis=1),\n",
    "            \"ids\": list(horse_id.iloc[:, 0].values),\n",
    "            \"date\": df_raw.loc[0, \"date\"],\n",
    "            \"label\": horse_info[\"3着以内\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([720])\n",
      "torch.Size([40])\n",
      "torch.Size([162])\n",
      "torch.Size([612])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     1.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "15    0.0\n",
       "16    0.0\n",
       "17    0.0\n",
       "Name: 3着以内, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# レース結果\n",
    "test = RaceResults.make_infos(\"../Raw-Data/Race-Results/2022/01020607.pkl\", multi=False)\n",
    "\n",
    "# 親成績\n",
    "test_ped_paths = [f\"../Raw-Data/Pedigree/{i}.pickle\" for i in test[\"ids\"]]\n",
    "test_ped = PedigreeResults.process(test_ped_paths)\n",
    "\n",
    "# 過去成績\n",
    "test_results_paths = [f\"../Raw-Data/Horse-Results/{i}.pkl\" for i in test[\"ids\"]]\n",
    "test_results = HorseResult.process(test_results_paths, test[\"date\"])\n",
    "vae = VAE(\"../models/horse_result_VAE.pth\", \"cpu\")\n",
    "vae_test = vae.process(test_results)\n",
    "vae_len = 18 - len(vae_test)\n",
    "for _ in range(vae_len):\n",
    "    array_zeros = torch.zeros(40)\n",
    "    vae_test.append(array_zeros)\n",
    "vae_test = torch.cat(vae_test)\n",
    "\n",
    "race_array = test[\"race\"].values.flatten()\n",
    "race = torch.tensor(race_array, dtype=torch.float32)\n",
    "horse_array = test[\"horse\"].values.flatten()\n",
    "horse = torch.tensor(horse_array, dtype=torch.float32)\n",
    "ped_array = test_ped.values.flatten()\n",
    "peds = torch.tensor(ped_array, dtype=torch.float32)\n",
    "\"\"\"\n",
    "race:1*40\n",
    "horse:18*9\n",
    "peds:18*44\n",
    "result:1*720 (18*4*10)\n",
    "\"\"\"\n",
    "print(vae_test.size())\n",
    "print(race.size())\n",
    "print(horse.size())\n",
    "print(peds.size())\n",
    "test[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習用データセットの作成\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモ  \n",
    "入力データの繋ぎ方は  \n",
    "1.レース情報  \n",
    "2.出走馬情報  \n",
    "3.過去成績  \n",
    "4.親成績  \n",
    "の順番でする。各情報を１次元の torch,Tensor 型にして結合する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Raw-Data/Race-Results/2015/01010101.pkl'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_to_tensor_1d(df_raw: pd.DataFrame) -> torch.Tensor:\n",
    "    df = df_raw.copy()\n",
    "    df_array = df.values.flatten()\n",
    "    return torch.tensor(df_array, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def add_tensor(tensors: list[torch.Tensor]) -> torch.Tensor:\n",
    "    tensors_tmp = tensors\n",
    "    add_num = 18 - len(tensors)\n",
    "    for _ in range(add_num):\n",
    "        array_zeros = torch.zeros(40)\n",
    "        tensors_tmp.append(array_zeros)\n",
    "    return torch.cat(tensors_tmp)\n",
    "\n",
    "\n",
    "def make_train_data(path: str, multi=True):\n",
    "    vae = VAE(\"../models/horse_result_VAE.pth\")\n",
    "    data = RaceResults.make_infos(path, multi=multi)\n",
    "\n",
    "    ped_paths = [f\"../Raw-Data/Pedigree/{i}.pickle\" for i in data[\"ids\"]]\n",
    "    results_paths = [f\"../Raw-Data/Horse-Results/{i}.pkl\" for i in data[\"ids\"]]\n",
    "\n",
    "    ped_raw = PedigreeResults.process(ped_paths)\n",
    "    result_raw = HorseResult.process(results_paths)\n",
    "\n",
    "    vae_raw = vae.process(result_raw)\n",
    "    vae_result = add_tensor(vae_raw)\n",
    "\n",
    "    race = df_to_tensor_1d(data[\"race\"])\n",
    "    horse = df_to_tensor_1d(data[\"horse\"])\n",
    "    peds = df_to_tensor_1d(ped_raw)\n",
    "    label = df_to_tensor_1d(data[\"label\"])\n",
    "    return {\n",
    "        \"label\": label,  # 正解ラベル\n",
    "        \"race\": race,  # レース情報\n",
    "        \"horse\": horse,  # 出走馬\n",
    "        \"results\": torch.cat([vae_result, peds]),  # 成績\n",
    "    }\n",
    "\n",
    "\n",
    "def save_pickle(path: str, data) -> None:\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def get_all_files_in_directory(path):\n",
    "    \"\"\"\n",
    "    指定したパスの配下にあるすべてのファイル（子孫ファイルも含む）のリストを返します。\n",
    "\n",
    "    :param path: ファイルを検索するディレクトリのパス\n",
    "    :return: ファイルのパスのリスト\n",
    "    \"\"\"\n",
    "    files_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_name in files:\n",
    "            files_list.append(f\"{root}/{file_name}\")\n",
    "    return files_list\n",
    "\n",
    "\n",
    "get_all_files_in_directory(\"../Raw-Data/Race-Results/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31093 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3611/31093 [1:54:45<14:33:22,  1.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_path_single):\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     result_processed \u001b[38;5;241m=\u001b[39m \u001b[43mmake_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     save_pickle(save_path_single, result_processed)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[23], line 23\u001b[0m, in \u001b[0;36mmake_train_data\u001b[1;34m(path, multi)\u001b[0m\n\u001b[0;32m     20\u001b[0m ped_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Raw-Data/Pedigree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     21\u001b[0m results_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Raw-Data/Horse-Results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m---> 23\u001b[0m ped_raw \u001b[38;5;241m=\u001b[39m \u001b[43mPedigreeResults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mped_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m result_raw \u001b[38;5;241m=\u001b[39m HorseResult\u001b[38;5;241m.\u001b[39mprocess(results_paths)\n\u001b[0;32m     26\u001b[0m vae_raw \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mprocess(result_raw)\n",
      "Cell \u001b[1;32mIn[20], line 144\u001b[0m, in \u001b[0;36mPedigreeResults.process\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    142\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m path:\n\u001b[1;32m--> 144\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    145\u001b[0m result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results)\n\u001b[0;32m    146\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[20], line 136\u001b[0m, in \u001b[0;36mPedigreeResults.transform\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    134\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodify(i)\n\u001b[0;32m    135\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtotalling_result(results_df))\n\u001b[1;32m--> 136\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mpca_transform(result)\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:393\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    380\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    381\u001b[0m     objs,\n\u001b[0;32m    382\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    391\u001b[0m )\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    678\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 680\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    684\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:587\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    584\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 587\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1749\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1749\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m _consolidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m   1751\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1731\u001b[0m, in \u001b[0;36mBlockManager.is_consolidated\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;124;03mReturn True if more than one block with the same dtype\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated:\n\u001b[1;32m-> 1731\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1740\u001b[0m, in \u001b[0;36mBlockManager._consolidate_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1740\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dtypes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(dtypes))\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1740\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1740\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m]\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dtypes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(dtypes))\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:196\u001b[0m, in \u001b[0;36mBlock._can_consolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_can_consolidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# We _could_ consolidate for DatetimeTZDtype but don't for now.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_extension\u001b[49m\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dokur\\Desktop\\src\\keibaAI\\py311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:187\u001b[0m, in \u001b[0;36mBlock.is_extension\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_object\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_can_consolidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# We _could_ consolidate for DatetimeTZDtype but don't for now.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir_list = get_all_files_in_directory(\"../Raw-Data/Race-Results/\")\n",
    "count = 0\n",
    "for i in tqdm(dir_list):\n",
    "    # 単勝で作る場合はRace-Results-singleフォルダ、make_train_dataのmultiの値をfalseにする\n",
    "    try:\n",
    "        save_path = f\"../Processed-Data/Race-Results/{count}.pickle\"\n",
    "        save_path_single = f\"../Processed-Data/Race-Results-single/{count}.pickle\"\n",
    "        count += 1\n",
    "        if os.path.exists(save_path_single):\n",
    "            continue\n",
    "        result_processed = make_train_data(i, multi=False)\n",
    "        save_pickle(save_path_single, result_processed)\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1534])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"../Processed-Data/Race-Results/0.pickle\"\n",
    "with open(test, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "data\n",
    "t_inputs = torch.cat([data[\"race\"], data[\"horse\"], data[\"results\"]])\n",
    "t_labels = data[\"label\"]\n",
    "t_inputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここまで\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
